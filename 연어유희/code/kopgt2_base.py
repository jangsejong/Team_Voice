import torch
from transformers import GPT2LMHeadModel
from transformers import PreTrainedTokenizerFast
tokenizer = PreTrainedTokenizerFast.from_pretrained("skt/kogpt2-base-v2", bos_token='</s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>') 
tokenizer.tokenize("ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ GPT-2 ì…ë‹ˆë‹¤.ğŸ˜¤:)l^o")

model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')

import numpy as np
import pandas as pd
import torch
from pytorch_lightning import Trainer
from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning.core.lightning import LightningModule
from torch.utils.data import DataLoader, Dataset
from transformers.optimization import AdamW, get_cosine_schedule_with_warmup
from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel
import re

Q_TKN = "<usr>"  # ì§ˆë¬¸
A_TKN = "<sys>"  # ëŒ€ë‹µ
BOS = '</s>'   # ì‹œì‘
EOS = '</s>'   # ë
MASK = '<unused0>'
SENT = '<unused1>'  # ê°ì •
PAD = '<pad>' 

koGPT2_TOKENIZER = PreTrainedTokenizerFast.from_pretrained("skt/kogpt2-base-v2",  
            bos_token=BOS, eos_token=EOS, unk_token='<unk>',
            pad_token=PAD, mask_token=MASK) 
model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')

path = 'C:\\Users\\bitcamp\\Desktop\\TP\\' 
Chatbot_Data = pd.read_csv(path + "ChatBotData_ì§€í¬.csv") 

Chatbot_Data = Chatbot_Data[:1100]
Chatbot_Data.head()

BOS = "</s>"     # ë¬¸ì¥ì˜ ì‹œì‘ì„ ë‚˜íƒ€ë‚´ëŠ” token
EOS = "</s>"     # ë¬¸ì¥ì˜ ëì„ ë‚˜íƒ€ë‚´ëŠ” token
PAD = "<pad>"    
MASK = "<unused0>" 

'''
bos_token : ë¬¸ì¥ì˜ ì‹œì‘ì„ ë‚˜íƒ€ë‚´ëŠ” token
eos_token : ë¬¸ì¥ì˜ ëì„ ë‚˜íƒ€ë‚´ëŠ” token
unk_token : ëª¨ë¥´ëŠ” ë‹¨ì–´ë¥¼ ë‚˜íƒ€ë‚´ëŠ” token
pad_token : ë™ì¼í•œ batch ë‚´ì—ì„œ ì…ë ¥ì˜ í¬ê¸°ë¥¼ ë™ì¼í•˜ê²Œ í•˜ê¸° ìœ„í•´ì„œ ì‚¬ìš©í•˜ëŠ” token

PreTrainedTokenizer ì—ì„œ ì œê³µë˜ëŠ” í•¨ìˆ˜ëŠ”

tokenize() : tokenizerë¥¼ ì´ìš©í•´ì„œ stringì„ token idì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•œë‹¤.
get_added_vocab() : token to indexì— í•´ë‹¹í•˜ëŠ” dictë¥¼ ë¦¬í„´í•œë‹¤.
batch_decode() : token idë¡œ êµ¬ì„±ëœ ì…ë ¥ì„ í•˜ë‚˜ì˜ ì—°ê²°ëœ stringìœ¼ë¡œ ì¶œë ¥í•œë‹¤.
convert_ids_to_tokens() : token id ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ tokenìœ¼ë¡œ ë³€í™˜í•œë‹¤. skip_special_tokens=Trueë¡œ í•˜ë©´ decodingí•  ë•Œ special tokenë“¤ì„ ì œê±°í•œë‹¤.
convert_tokens_to_ids() : token stringì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ token id ë˜ëŠ” Token idì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•œë‹¤.
decode() : tokenizer ì™€ vocabularyë¥¼ ì´ìš©í•´ì„œ token idë¥¼ stringìœ¼ë¡œ ë³€í™˜í•œë‹¤. skip_special_token=Trueë¡œ ì§€ì •í•˜ë©´ speical tokenë“¤ì„ ì œì™¸í•œë‹¤.
encode() : token stringì„ token id ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•œë‹¤. add_special_tokens=Falseë¡œ ì§€ì •í•˜ë©´ token idë¡œ ë³€í™˜í•  ë•Œ special tokenë“¤ì„ ì œì™¸í•œë‹¤. 
paddingì„ í†µí•´ì„œ padding tokenì„ ì–´ë–»ê²Œ ì¶”ê°€í• ì§€ë„ ì§€ì •í•  ìˆ˜ ìˆë‹¤.
'''

koGPT2_TOKENIZER = PreTrainedTokenizerFast.from_pretrained("skt/kogpt2-base-v2", bos_token=BOS, eos_token=EOS, unk_token="<unk>", pad_token=PAD, mask_token=MASK,)

class ChatbotDataset(Dataset): # ìë™ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” í´ë˜ìŠ¤
    def __init__(self, chats, max_len=40):  # ë°ì´í„°ì…‹ì˜ ì „ì²˜ë¦¬ë¥¼ í•´ì£¼ëŠ” ë¶€ë¶„
        self._data = chats     # ì±—ë´‡ ë°ì´í„°
        self.max_len = max_len # ìµœëŒ€ ê¸¸ì´ë¥¼ ì €ì¥í•œë‹¤.
        self.q_token = Q_TKN   # ì§ˆë¬¸
        self.a_token = A_TKN   # ëŒ€ë‹µ
        self.sent_token = SENT # ê°ì •
        self.eos = EOS         # ë¬¸ì¥ì˜ ëì„ ë‚˜íƒ€ë‚´ëŠ” token
        self.mask = MASK       # ë§ˆìŠ¤í¬ë¥¼ ë‚˜íƒ€ë‚´ëŠ” token
        self.tokenizer = koGPT2_TOKENIZER    # ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ë¶€ë¶„

    def __len__(self):  # chatbotdata ì˜ ê¸¸ì´ë¥¼ ë¦¬í„´í•œë‹¤.
        return len(self._data) # ë°ì´í„°ì˜ ê¸¸ì´ë¥¼ ë¦¬í„´í•œë‹¤.
    
    def __getitem__(self, idx):  # ë¡œë“œí•œ ì±—ë´‡ ë°ì´í„°ë¥¼ ì°¨ë¡€ì°¨ë¡€ DataLoaderë¡œ ë„˜ê²¨ì£¼ëŠ” ë©”ì„œë“œ
        turn = self._data.iloc[idx] # ì±—ë´‡ ë°ì´í„°ì˜ ì¸ë±ìŠ¤ë¥¼ ê°€ì ¸ì˜¨ë‹¤.
        q = turn["Q"]  # ì§ˆë¬¸ì„ ê°€ì ¸ì˜¨ë‹¤.
        q = re.sub(r"([?.!,])", r" ", q)  # êµ¬ë‘£ì ë“¤ì„ ì œê±°í•œë‹¤.

        a = turn["A"]  # ë‹µë³€ì„ ê°€ì ¸ì˜¨ë‹¤.
        a = re.sub(r"([?.!,])", r" ", a)  # êµ¬ë‘£ì ë“¤ì„ ì œê±°í•œë‹¤.

        q_toked = self.tokenizer.tokenize(self.q_token + q + self.sent_token)  
        # ì§ˆë¬¸ + ê°ì • # ì§ˆë¬¸ì„ í† í¬ë‚˜ì´ì§•í•œë‹¤.
        q_len = len(q_toked) # ì§ˆë¬¸ì˜ ê¸¸ì´ë¥¼ êµ¬í•œë‹¤.

        a_toked = self.tokenizer.tokenize(self.a_token + a + self.eos) 
        # ëŒ€ë‹µ + ë # ëŒ€ë‹µì„ í† í¬ë‚˜ì´ì§•í•œë‹¤.
        a_len = len(a_toked) # ëŒ€ë‹µì˜ ê¸¸ì´ë¥¼ êµ¬í•œë‹¤.

        if q_len > self.max_len: # ì§ˆë¬¸ì˜ ê¸¸ì´ê°€ ìµœëŒ€ê¸¸ì´ë³´ë‹¤ í¬ë©´
            a_len = self.max_len - q_len        #ë‹µë³€ì˜ ê¸¸ì´ë¥¼ ìµœëŒ€ê¸¸ì´ - ì§ˆë¬¸ê¸¸ì´
            if a_len <= 0:       #ì§ˆë¬¸ì˜ ê¸¸ì´ê°€ ë„ˆë¬´ ê¸¸ì–´ ì§ˆë¬¸ë§Œìœ¼ë¡œ ìµœëŒ€ ê¸¸ì´ë¥¼ ì´ˆê³¼ í•œë‹¤ë©´
                q_toked = q_toked[-(int(self.max_len / 2)) :]   #ì§ˆë¬¸ê¸¸ì´ë¥¼ ìµœëŒ€ê¸¸ì´ì˜ ë°˜ìœ¼ë¡œ 
                q_len = len(q_toked)                         #ì§ˆë¬¸ì˜ ê¸¸ì´ë¥¼ êµ¬í•œë‹¤.
                a_len = self.max_len - q_len                 #ë‹µë³€ì˜ ê¸¸ì´ë¥¼ ìµœëŒ€ê¸¸ì´ - ì§ˆë¬¸ê¸¸ì´  
            a_toked = a_toked[:a_len]                        #ë‹µë³€ì˜ ê¸¸ì´ë§Œí¼ ëŒ€ë‹µì„ ê°€ì ¸ì˜¨ë‹¤.
            a_len = len(a_toked)                             #ë‹µë³€ì˜ ê¸¸ì´ë¥¼ êµ¬í•œë‹¤.

        if q_len + a_len > self.max_len:                #ì§ˆë¬¸ê¸¸ì´ + ë‹µë³€ê¸¸ì´ê°€ ìµœëŒ€ê¸¸ì´ë³´ë‹¤ í¬ë©´
            a_len = self.max_len - q_len        #ë‹µë³€ì˜ ê¸¸ì´ë¥¼ ìµœëŒ€ê¸¸ì´ - ì§ˆë¬¸ê¸¸ì´
            if a_len <= 0:       #ì§ˆë¬¸ì˜ ê¸¸ì´ê°€ ë„ˆë¬´ ê¸¸ì–´ ì§ˆë¬¸ë§Œìœ¼ë¡œ ìµœëŒ€ ê¸¸ì´ë¥¼ ì´ˆê³¼ í•œë‹¤ë©´
                q_toked = q_toked[-(int(self.max_len / 2)) :]   #ì§ˆë¬¸ê¸¸ì´ë¥¼ ìµœëŒ€ê¸¸ì´ì˜ ë°˜ìœ¼ë¡œ 
                q_len = len(q_toked)                        # ì§ˆë¬¸ì˜ ê¸¸ì´ë¥¼ êµ¬í•œë‹¤.
                a_len = self.max_len - q_len              #ë‹µë³€ì˜ ê¸¸ì´ë¥¼ ìµœëŒ€ê¸¸ì´ - ì§ˆë¬¸ê¸¸ì´
            a_toked = a_toked[:a_len]               #ë‹µë³€ì˜ ê¸¸ì´ë§Œí¼ ëŒ€ë‹µì„ ê°€ì ¸ì˜¨ë‹¤.
            a_len = len(a_toked)                    # ë‹µë³€ì˜ ê¸¸ì´ë¥¼ êµ¬í•œë‹¤.
            
# ë‹µë³€ labels = [mask, mask, ...., mask, ..., <bos>,..ë‹µë³€.. <eos>, <pad>....] 
# ë‹µë³€ì˜ ê¸¸ì´ë§Œí¼ ë§ˆìŠ¤í¬ë¥¼ ì¶”ê°€í•œë‹¤.
        labels = [self.mask,] * q_len + a_toked[1:] # ë§ˆìŠ¤í¬ë¥¼ ì§ˆë¬¸ê¸¸ì´ë§Œí¼ ë„£ì–´ì¤€ë‹¤.

        # mask = ì§ˆë¬¸ê¸¸ì´ 0 + ë‹µë³€ê¸¸ì´ 1 + ë‚˜ë¨¸ì§€ 0
        mask = [0] * q_len + [1] * a_len + [0] * (self.max_len - q_len - a_len) # ë‹µë³€ì˜ ê¸¸ì´ë§Œí¼ 1ì„ ë„£ì–´ì¤€ë‹¤.
        # ë‹µë³€ labelsì„ index ë¡œ ë§Œë“ ë‹¤.
        labels_ids = self.tokenizer.convert_tokens_to_ids(labels) # ë‹µë³€ì„ indexë¡œ ë³€í™˜í•œë‹¤.    
        # ìµœëŒ€ê¸¸ì´ë§Œí¼ PADDING
        while len(labels_ids) < self.max_len:
            labels_ids += [self.tokenizer.pad_token_id]

        # ì§ˆë¬¸ + ë‹µë³€ì„ index ë¡œ ë§Œë“ ë‹¤.    
        token_ids = self.tokenizer.convert_tokens_to_ids(q_toked + a_toked) # ì§ˆë¬¸ê³¼ ë‹µë³€ì„ indexë¡œ ë³€í™˜í•œë‹¤.
        # ìµœëŒ€ê¸¸ì´ë§Œí¼ PADDING
        while len(token_ids) < self.max_len: # ì§ˆë¬¸ê¸¸ì´ + ë‹µë³€ê¸¸ì´ë§Œí¼ ë„£ì–´ì¤€ë‹¤.
            token_ids += [self.tokenizer.pad_token_id] # PADDING
 
        #ì§ˆë¬¸+ë‹µë³€, ë§ˆìŠ¤í¬, ë‹µë³€
        return (token_ids, np.array(mask), labels_ids) # ë‹µë³€ì„ indexë¡œ ë³€í™˜í•œë‹¤.
    
def collate_batch(batch):   #  batchë¥¼ ë°›ì•„ì„œ ë°°ì¹˜ë¥¼ ë§Œë“ ë‹¤.
    data = [item[0] for item in batch]   # batchì—ì„œ dataë§Œ ë½‘ì•„ì„œ ë°°ì—´ë¡œ ë§Œë“ ë‹¤.
    mask = [item[1] for item in batch]   # batchì—ì„œ maskë§Œ ë½‘ì•„ì„œ ë°°ì—´ë¡œ ë§Œë“ ë‹¤.
    label = [item[2] for item in batch]  # batchì—ì„œ labelë§Œ ë½‘ì•„ì„œ ë°°ì—´ë¡œ ë§Œë“ ë‹¤.
    return torch.LongTensor(data), torch.LongTensor(mask), torch.LongTensor(label) # ë°°ì—´ë¡œ ë§Œë“ ë‹¤.
    
train_set = ChatbotDataset(Chatbot_Data, max_len=40)

#ìœˆë„ìš° í™˜ê²½ì—ì„œ num_workers ëŠ” ë¬´ì¡°ê±´ 0ìœ¼ë¡œ ì§€ì •, ë¦¬ëˆ…ìŠ¤ì—ì„œëŠ” 2
train_dataloader = DataLoader(train_set, batch_size=32, num_workers=0, shuffle=True, collate_fn=collate_batch,)
            
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
train_set = ChatbotDataset(Chatbot_Data, max_len=40)
#ìœˆë„ìš° í™˜ê²½ì—ì„œ num_workers ëŠ” ë¬´ì¡°ê±´ 0ìœ¼ë¡œ ì§€ì •, ë¦¬ëˆ…ìŠ¤ì—ì„œëŠ” 2
train_dataloader = DataLoader(train_set, batch_size=32, num_workers=0, shuffle=True, collate_fn=collate_batch,)

model.to(device)
model.train()

learning_rate = 3e-5
criterion = torch.nn.CrossEntropyLoss(reduction="none")  # ê¸°ì¤€
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

epoch = 10
Sneg = -1e18                 
            
print ("start")
for epoch in range(epoch):
    for batch_idx, samples in enumerate(train_dataloader):
        optimizer.zero_grad()
        token_ids, mask, label = samples
        out = model(token_ids)
        out = out.logits      #Returns a new tensor with the logit of the elements of input
        mask_3d = mask.unsqueeze(dim=2).repeat_interleave(repeats=out.shape[2], dim=2)   # unsqueezeí•¨ìˆ˜ëŠ” squeezeí•¨ìˆ˜ì˜ ë°˜ëŒ€ë¡œ 1ì¸ ì°¨ì›ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ì´ë‹¤. ê·¸ë˜ì„œ ì–´ëŠ ì°¨ì›ì— 1ì¸ ì°¨ì›ì„ ìƒì„±í•  ì§€ ê¼­ ì§€ì •í•´ì£¼ì–´ì•¼í•œë‹¤.
        mask_out = torch.where(mask_3d == 1, out, Sneg * torch.ones_like(out))
        loss = criterion(mask_out.transpose(2, 1), label)
        # í‰ê·  loss ë§Œë“¤ê¸° avg_loss[0] / avg_loss[1] <- loss ì •ê·œí™”
        avg_loss = loss.sum() / mask.sum()
        avg_loss.backward()
        # í•™ìŠµ ë
        optimizer.step()
print ("end")

sent = '0'
with torch.no_grad():
    while 1:
        q = input("user > ").strip()   # ì¸ìë¡œ ì „ë‹¬ëœ ë¬¸ìë¥¼ Stringì˜ ì™¼ìª½ê³¼ ì˜¤ë¥¸ìª½ì—ì„œ ì œê±°í•©ë‹ˆë‹¤.
        if q == "quit":  
            break
        a = ""
        while 1:
            input_ids = torch.LongTensor(koGPT2_TOKENIZER.encode(Q_TKN + q + SENT + sent + A_TKN + a)).unsqueeze(dim=0)
            pred = model(input_ids)
            pred = pred.logits
            gen = koGPT2_TOKENIZER.convert_ids_to_tokens(torch.argmax(pred, dim=-1).squeeze().numpy().tolist())[-1]
            if gen == EOS:
                break
            a += gen.replace("â–", " ")
        print("Chatbot > {}".format(a.strip()))



        
